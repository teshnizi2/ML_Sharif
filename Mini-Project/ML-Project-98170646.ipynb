{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME: Mohammadreza Ahmadi Teshnizi\n",
        "Student number: 98170646\n",
        "Colab link:https://colab.research.google.com/drive/1BqdjoZS17_3q4s_wiAwFQopFVKNbvX3B?usp=sharing"
      ],
      "metadata": {
        "id": "xKYZPPtg0SDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H8c97UuZ0WJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "9Q1fVN2g2vp_",
        "outputId": "f17904a8-1bcc-4b90-c0ad-a6f93fd867ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1BRhlDz-JASvfEljf_XxrL-ZH4gUvOg0v'\n",
        "output =  'data-train.csv'\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "data = pd.read_csv('data-train.csv')\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TubM-tHS3MdX",
        "outputId": "372a3a4f-47f0-4461-fd8a-8301afd8cb20"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BRhlDz-JASvfEljf_XxrL-ZH4gUvOg0v\n",
            "To: /content/data-train.csv\n",
            "100%|██████████| 8.48M/8.48M [00:00<00:00, 231MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PhraseId  SentenceId                                             Phrase  \\\n",
            "0         1           1  A series of escapades demonstrating the adage ...   \n",
            "1         2           1  A series of escapades demonstrating the adage ...   \n",
            "2         3           1                                           A series   \n",
            "3         4           1                                                  A   \n",
            "4         5           1                                             series   \n",
            "\n",
            "   Sentiment  \n",
            "0          1  \n",
            "1          2  \n",
            "2          2  \n",
            "3          2  \n",
            "4          2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training data: {train_data.shape}\")\n",
        "print(f\"Validation data: {valid_data.shape}\")\n",
        "print(f\"Testing data: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYeZ6ZV7_-QB",
        "outputId": "552c21cb-ae02-4270-a537-c488c31255c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: (124038, 4)\n",
            "Validation data: (15505, 4)\n",
            "Testing data: (15505, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_data_basic(data):\n",
        "    data = data.lower()\n",
        "    data = re.sub(r'[^\\w\\s]', '', data)\n",
        "    data = ' '.join(stemmer.stem(word) for word in data.split() if word not in stop_words)\n",
        "    return data\n",
        "\n",
        "train_data['Phrase'] = train_data['Phrase'].apply(preprocess_data_basic)\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_features = vectorizer.fit_transform(train_data['Phrase'])"
      ],
      "metadata": {
        "id": "yTC-6GbtACG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d33e731-9fa3-4168-db63-55a4dfae2968"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data_w2v(data):\n",
        "    data = data.lower()\n",
        "    data = re.sub(r'[^\\w\\s]', '', data).split()\n",
        "    data = [word for word in data if word not in stop_words]\n",
        "    return data\n",
        "\n",
        "train_data_w2v = train_data['Phrase'].apply(preprocess_data_w2v)\n",
        "\n",
        "model = Word2Vec(sentences=train_data_w2v, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def phrase_vector(phrase):\n",
        "    phrase_vec = []\n",
        "    for word in phrase:\n",
        "        try:\n",
        "            phrase_vec.append(model.wv[word])\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if phrase_vec:\n",
        "        return sum(phrase_vec)/len(phrase_vec)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "train_features_w2v = np.array([phrase_vector(phrase) for phrase in train_data_w2v])"
      ],
      "metadata": {
        "id": "iPiQMhoNGn-R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = train_data['Sentiment'].value_counts()\n",
        "print(sentiment_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ignHTgPjGn7j",
        "outputId": "82f7429a-96be-43a0-ea6b-40642cb9ccca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    63330\n",
            "3    26157\n",
            "1    21622\n",
            "4     7288\n",
            "0     5641\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100]}\n",
        "\n",
        "# Approach 1: Basic Methods\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=100000), param_grid, refit=True, verbose=3)\n",
        "grid.fit(train_features, train_data['Sentiment'])\n",
        "\n",
        "valid_predictions_basic = grid.predict(vectorizer.transform(valid_data['Phrase']))\n",
        "\n",
        "print(\"Approach 1: Basic Methods\")\n",
        "print(classification_report(valid_data['Sentiment'], valid_predictions_basic))\n",
        "\n",
        "# Approach 2: Word2Vec\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=100000), param_grid, refit=True, verbose=3)\n",
        "grid.fit(train_features_w2v, train_data['Sentiment'])\n",
        "\n",
        "valid_predictions_w2v = grid.predict([phrase_vector(phrase) for phrase in valid_data['Phrase'].apply(preprocess_data_w2v)])\n",
        "\n",
        "print(\"\\nApproach 2: Word2Vec\")\n",
        "print(classification_report(valid_data['Sentiment'], valid_predictions_w2v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es0BAFiWGn5B",
        "outputId": "c7328eba-0247-47ef-aa99-1d75233d7f56"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "[CV 1/5] END .............................C=0.1;, score=0.565 total time=   9.8s\n",
            "[CV 2/5] END .............................C=0.1;, score=0.564 total time=   8.7s\n",
            "[CV 3/5] END .............................C=0.1;, score=0.562 total time=   9.5s\n",
            "[CV 4/5] END .............................C=0.1;, score=0.561 total time=   8.6s\n",
            "[CV 5/5] END .............................C=0.1;, score=0.563 total time=  10.2s\n",
            "[CV 1/5] END ...............................C=1;, score=0.624 total time=  25.9s\n",
            "[CV 2/5] END ...............................C=1;, score=0.626 total time=  27.9s\n",
            "[CV 3/5] END ...............................C=1;, score=0.620 total time=  29.6s\n",
            "[CV 4/5] END ...............................C=1;, score=0.624 total time=  29.1s\n",
            "[CV 5/5] END ...............................C=1;, score=0.627 total time=  27.0s\n",
            "[CV 1/5] END ..............................C=10;, score=0.635 total time= 1.1min\n",
            "[CV 2/5] END ..............................C=10;, score=0.638 total time= 1.1min\n",
            "[CV 3/5] END ..............................C=10;, score=0.632 total time= 1.2min\n",
            "[CV 4/5] END ..............................C=10;, score=0.634 total time= 1.1min\n",
            "[CV 5/5] END ..............................C=10;, score=0.635 total time= 1.0min\n",
            "[CV 1/5] END .............................C=100;, score=0.626 total time= 2.4min\n",
            "[CV 2/5] END .............................C=100;, score=0.632 total time= 2.5min\n",
            "[CV 3/5] END .............................C=100;, score=0.623 total time= 2.7min\n",
            "[CV 4/5] END .............................C=100;, score=0.629 total time= 2.7min\n",
            "[CV 5/5] END .............................C=100;, score=0.628 total time= 2.6min\n",
            "Approach 1: Basic Methods\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.09      0.14       680\n",
            "           1       0.46      0.14      0.22      2680\n",
            "           2       0.56      0.93      0.70      7917\n",
            "           3       0.44      0.19      0.26      3295\n",
            "           4       0.52      0.08      0.14       933\n",
            "\n",
            "    accuracy                           0.55     15505\n",
            "   macro avg       0.49      0.28      0.29     15505\n",
            "weighted avg       0.51      0.55      0.47     15505\n",
            "\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "[CV 1/5] END .............................C=0.1;, score=0.516 total time=  30.1s\n",
            "[CV 2/5] END .............................C=0.1;, score=0.517 total time=  30.4s\n",
            "[CV 3/5] END .............................C=0.1;, score=0.514 total time=  28.7s\n",
            "[CV 4/5] END .............................C=0.1;, score=0.517 total time=  31.1s\n",
            "[CV 5/5] END .............................C=0.1;, score=0.516 total time=  30.6s\n",
            "[CV 1/5] END ...............................C=1;, score=0.518 total time= 1.2min\n",
            "[CV 2/5] END ...............................C=1;, score=0.520 total time= 1.1min\n",
            "[CV 3/5] END ...............................C=1;, score=0.517 total time= 1.0min\n",
            "[CV 4/5] END ...............................C=1;, score=0.517 total time= 1.2min\n",
            "[CV 5/5] END ...............................C=1;, score=0.515 total time= 1.2min\n",
            "[CV 1/5] END ..............................C=10;, score=0.517 total time= 1.8min\n",
            "[CV 2/5] END ..............................C=10;, score=0.520 total time= 1.7min\n",
            "[CV 3/5] END ..............................C=10;, score=0.517 total time= 1.8min\n",
            "[CV 4/5] END ..............................C=10;, score=0.517 total time= 1.8min\n",
            "[CV 5/5] END ..............................C=10;, score=0.515 total time= 1.7min\n",
            "[CV 1/5] END .............................C=100;, score=0.517 total time= 1.9min\n",
            "[CV 2/5] END .............................C=100;, score=0.520 total time= 2.0min\n",
            "[CV 3/5] END .............................C=100;, score=0.517 total time= 2.1min\n",
            "[CV 4/5] END .............................C=100;, score=0.517 total time= 2.0min\n",
            "[CV 5/5] END .............................C=100;, score=0.515 total time= 1.7min\n",
            "\n",
            "Approach 2: Word2Vec\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.01      0.02       680\n",
            "           1       0.32      0.05      0.09      2680\n",
            "           2       0.53      0.92      0.67      7917\n",
            "           3       0.39      0.14      0.20      3295\n",
            "           4       0.27      0.02      0.04       933\n",
            "\n",
            "    accuracy                           0.51     15505\n",
            "   macro avg       0.35      0.23      0.21     15505\n",
            "weighted avg       0.44      0.51      0.41     15505\n",
            "\n"
          ]
        }
      ]
    }
  ]
}